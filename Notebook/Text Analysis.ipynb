{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MongoDB Connection</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDBConnection():\n",
    "    client = MongoClient(\"localhost:27017\")\n",
    "    db=client.articles\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=getDBConnection()\n",
    "articles=db.Articles\n",
    "row=articles.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Okay Just Print First Record<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5c2a2e6b986d090428970616'),\n",
      " 'author': 'newsfeedback@fool.com (Sean Williams)',\n",
      " 'content': 'The marijuana industry has had an absolutely game-changing year '\n",
      "            'in 2018. Canada became the first industrialized country in the '\n",
      "            'world to green-light recreational marijuana, and a handful of '\n",
      "            'U.S. states legalized cannabis in some capacity. In other words, '\n",
      "            'the pâ€¦ [+8176 chars]',\n",
      " 'description': 'These pot stocks are likely to create all the buzz in the new '\n",
      "                'year.',\n",
      " 'publishedAt': datetime.datetime(2018, 12, 31, 13, 21),\n",
      " 'source_id': 'the-motely-fool',\n",
      " 'summarization': 'Although the legal cannabis market is still exceptionally '\n",
      "                  'young and unproven, here - in no particular order - are 12 '\n",
      "                  'pot-growing stocks you should be watching in 2019. The '\n",
      "                  'company currently has 4.3 million square feet of licensed '\n",
      "                  'production capacity, and anticipates having all 5.6 million '\n",
      "                  'square feet licensed by the end of 2019. Expect 2019 to be '\n",
      "                  'the year that Aurora finds a beverage, tobacco, or '\n",
      "                  \"pharmaceutical partner, and don't be surprised if the \"\n",
      "                  \"company's acquisition binge continues. What we do know is \"\n",
      "                  'that if management meets its production guidance, Aphria '\n",
      "                  'will slot in as the third-largest grower by annual yield at '\n",
      "                  '255,000 kilograms. Tilray has close to 3 million square '\n",
      "                  'feet in growing capacity that it could develop, which makes '\n",
      "                  'it a wild card in terms of peak production. The company '\n",
      "                  'anticipates completing the final of three stages of its '\n",
      "                  'phase 4 expansion at the Moncton, New Brunswick, facility '\n",
      "                  'by October 2019. Despite only 342,000 square feet of '\n",
      "                  'growing space, Supreme Cannabis has visions of generating '\n",
      "                  '50,000 kilograms of weed per year.',\n",
      " 'title': '12 Marijuana Growing Stocks You Need to Know for 2019',\n",
      " 'url': 'https://www.fool.com/investing/2018/12/31/12-marijuana-growing-stocks-you-need-to-know-for-2.aspx',\n",
      " 'urlToImage': 'https://g.foolcdn.com/image/?url=https%3A%2F%2Fg.foolcdn.com%2Feditorial%2Fimages%2F505294%2Fcannabis-jars-marijuana-pot-weed-canada-legal-getty.jpg&h=630&w=1200&op=resize'}\n"
     ]
    }
   ],
   "source": [
    "pprint(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "106904"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extract summarisation column from article table</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "myresult = articles.find().limit(10000)\n",
    "dataset=[]\n",
    "for x in myresult:\n",
    "    try:\n",
    "        dataset.append(x[\"summarization\"])\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset Ready</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features=5000\n",
    "no_topics = 3\n",
    "no_top_words = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Although the legal cannabis market is still exceptionally young and unproven, here - in no particular order - are 12 pot-growing stocks you should be watching in 2019. The company currently has 4.3 million square feet of licensed production capacity, and anticipates having all 5.6 million square feet licensed by the end of 2019. Expect 2019 to be the year that Aurora finds a beverage, tobacco, or pharmaceutical partner, and don't be surprised if the company's acquisition binge continues. What we do know is that if management meets its production guidance, Aphria will slot in as the third-largest grower by annual yield at 255,000 kilograms. Tilray has close to 3 million square feet in growing capacity that it could develop, which makes it a wild card in terms of peak production. The company anticipates completing the final of three stages of its phase 4 expansion at the Moncton, New Brunswick, facility by October 2019. Despite only 342,000 square feet of growing space, Supreme Cannabis has visions of generating 50,000 kilograms of weed per year.\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create the Document-Word matrix</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,stop_words='english',lowercase=True,\n",
    "                        strip_accents='ascii',analyzer = 'word',token_pattern='[a-zA-Z0-9]{3,}')\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '000kg',\n",
       " '001',\n",
       " '012',\n",
       " '018',\n",
       " '025',\n",
       " '030',\n",
       " '035',\n",
       " '036',\n",
       " '038',\n",
       " '040',\n",
       " '042',\n",
       " '044',\n",
       " '045',\n",
       " '048',\n",
       " '050',\n",
       " '055',\n",
       " '059',\n",
       " '060',\n",
       " '070',\n",
       " '072',\n",
       " '079',\n",
       " '083',\n",
       " '095',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100kg',\n",
       " '100ml',\n",
       " '101',\n",
       " '1011',\n",
       " '1015',\n",
       " '102',\n",
       " '103',\n",
       " '104',\n",
       " '105',\n",
       " '1057',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '109',\n",
       " '1092',\n",
       " '10am',\n",
       " '10mg',\n",
       " '10ml',\n",
       " '10pm',\n",
       " '10th',\n",
       " '110',\n",
       " '1100',\n",
       " '111',\n",
       " '112',\n",
       " '1127',\n",
       " '113',\n",
       " '1130',\n",
       " '11357',\n",
       " '11361',\n",
       " '1137',\n",
       " '114',\n",
       " '115',\n",
       " '115th',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '11am',\n",
       " '11pm',\n",
       " '11th',\n",
       " '120',\n",
       " '1200',\n",
       " '121',\n",
       " '1214',\n",
       " '122',\n",
       " '1227',\n",
       " '123',\n",
       " '124',\n",
       " '1244',\n",
       " '125',\n",
       " '1251',\n",
       " '1258',\n",
       " '126',\n",
       " '1263',\n",
       " '127',\n",
       " '128',\n",
       " '1285',\n",
       " '1286',\n",
       " '129',\n",
       " '1294',\n",
       " '12pm',\n",
       " '12th',\n",
       " '130',\n",
       " '1302',\n",
       " '131',\n",
       " '1313',\n",
       " '132',\n",
       " '133',\n",
       " '1331',\n",
       " '135',\n",
       " '1351',\n",
       " '136',\n",
       " '137',\n",
       " '138',\n",
       " '139',\n",
       " '1392',\n",
       " '13th',\n",
       " '140',\n",
       " '1400',\n",
       " '1409',\n",
       " '141',\n",
       " '142',\n",
       " '1420',\n",
       " '14272',\n",
       " '143',\n",
       " '144',\n",
       " '145',\n",
       " '1459',\n",
       " '146',\n",
       " '1460',\n",
       " '147',\n",
       " '148',\n",
       " '149',\n",
       " '14th',\n",
       " '150',\n",
       " '1500',\n",
       " '150th',\n",
       " '151',\n",
       " '1528',\n",
       " '153',\n",
       " '154',\n",
       " '1544',\n",
       " '155',\n",
       " '1550',\n",
       " '1554',\n",
       " '156',\n",
       " '157',\n",
       " '15740',\n",
       " '1578',\n",
       " '158',\n",
       " '159',\n",
       " '15th',\n",
       " '160',\n",
       " '1600',\n",
       " '1605',\n",
       " '161',\n",
       " '162',\n",
       " '163',\n",
       " '165',\n",
       " '166',\n",
       " '167',\n",
       " '168',\n",
       " '169',\n",
       " '16th',\n",
       " '170',\n",
       " '17025',\n",
       " '171',\n",
       " '172',\n",
       " '174',\n",
       " '1749',\n",
       " '175',\n",
       " '176',\n",
       " '177',\n",
       " '1775',\n",
       " '178',\n",
       " '1784',\n",
       " '1793',\n",
       " '17th',\n",
       " '180',\n",
       " '182',\n",
       " '183',\n",
       " '185',\n",
       " '186',\n",
       " '187',\n",
       " '188',\n",
       " '189',\n",
       " '18th',\n",
       " '190',\n",
       " '1900',\n",
       " '1900s',\n",
       " '1902',\n",
       " '1906',\n",
       " '1910s',\n",
       " '1917',\n",
       " '1919',\n",
       " '192',\n",
       " '1920s',\n",
       " '1923',\n",
       " '193',\n",
       " '1930',\n",
       " '1930s',\n",
       " '1933',\n",
       " '1934',\n",
       " '1936',\n",
       " '1937',\n",
       " '194',\n",
       " '1940',\n",
       " '1940s',\n",
       " '1943',\n",
       " '1944',\n",
       " '1946',\n",
       " '1948',\n",
       " '195',\n",
       " '1950s',\n",
       " '1957',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1961',\n",
       " '1964',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '197',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1974',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '198',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1982',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1990',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '19th',\n",
       " '1bn',\n",
       " '1st',\n",
       " '200',\n",
       " '2000',\n",
       " '2000s',\n",
       " '2001',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '201',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '202',\n",
       " '2020',\n",
       " '2021',\n",
       " '2022',\n",
       " '2023',\n",
       " '2024',\n",
       " '2025',\n",
       " '2026',\n",
       " '203',\n",
       " '2030',\n",
       " '2036',\n",
       " '205',\n",
       " '206',\n",
       " '2064',\n",
       " '2069',\n",
       " '207',\n",
       " '208',\n",
       " '209',\n",
       " '20pm',\n",
       " '20s',\n",
       " '20th',\n",
       " '210',\n",
       " '2100',\n",
       " '211',\n",
       " '212',\n",
       " '2121',\n",
       " '213',\n",
       " '214',\n",
       " '215',\n",
       " '216',\n",
       " '218',\n",
       " '219',\n",
       " '2197',\n",
       " '21st',\n",
       " '220',\n",
       " '221',\n",
       " '2215',\n",
       " '222',\n",
       " '225',\n",
       " '226',\n",
       " '227',\n",
       " '228',\n",
       " '229',\n",
       " '22nd',\n",
       " '22news',\n",
       " '230',\n",
       " '2300',\n",
       " '231',\n",
       " '232',\n",
       " '233',\n",
       " '234',\n",
       " '235',\n",
       " '238',\n",
       " '23rd',\n",
       " '240',\n",
       " '2402',\n",
       " '242',\n",
       " '243',\n",
       " '244',\n",
       " '245',\n",
       " '246',\n",
       " '247',\n",
       " '2477',\n",
       " '248',\n",
       " '249',\n",
       " '24th',\n",
       " '250',\n",
       " '2500',\n",
       " '251',\n",
       " '252',\n",
       " '255',\n",
       " '257',\n",
       " '25mg',\n",
       " '25th',\n",
       " '260',\n",
       " '261',\n",
       " '262',\n",
       " '263',\n",
       " '264',\n",
       " '2641',\n",
       " '265',\n",
       " '266',\n",
       " '2679',\n",
       " '269',\n",
       " '26th',\n",
       " '270',\n",
       " '271',\n",
       " '2727',\n",
       " '2729',\n",
       " '2733',\n",
       " '274',\n",
       " '275',\n",
       " '276',\n",
       " '277',\n",
       " '279',\n",
       " '27th',\n",
       " '280',\n",
       " '280e',\n",
       " '281',\n",
       " '283',\n",
       " '284',\n",
       " '285',\n",
       " '286',\n",
       " '287',\n",
       " '288',\n",
       " '28th',\n",
       " '290',\n",
       " '291',\n",
       " '2914',\n",
       " '292',\n",
       " '293',\n",
       " '295',\n",
       " '296',\n",
       " '297',\n",
       " '29th',\n",
       " '2mg',\n",
       " '2nd',\n",
       " '2pm',\n",
       " '300',\n",
       " '3000',\n",
       " '301',\n",
       " '302',\n",
       " '304',\n",
       " '305',\n",
       " '306',\n",
       " '307',\n",
       " '309',\n",
       " '30am',\n",
       " '30g',\n",
       " '30m',\n",
       " '30pm',\n",
       " '30s',\n",
       " '30th',\n",
       " '310',\n",
       " '311',\n",
       " '312',\n",
       " '313',\n",
       " '314',\n",
       " '315',\n",
       " '3157',\n",
       " '316',\n",
       " '318',\n",
       " '31st',\n",
       " '320',\n",
       " '321',\n",
       " '323',\n",
       " '325',\n",
       " '326',\n",
       " '327',\n",
       " '328',\n",
       " '32nd',\n",
       " '330',\n",
       " '332',\n",
       " '333',\n",
       " '335',\n",
       " '336',\n",
       " '340',\n",
       " '3400',\n",
       " '341',\n",
       " '342',\n",
       " '3441',\n",
       " '345',\n",
       " '349',\n",
       " '34th',\n",
       " '350',\n",
       " '3505',\n",
       " '351',\n",
       " '3521',\n",
       " '353',\n",
       " '3530',\n",
       " '354',\n",
       " '355',\n",
       " '356',\n",
       " '357',\n",
       " '358',\n",
       " '360',\n",
       " '365',\n",
       " '366',\n",
       " '367',\n",
       " '368',\n",
       " '370',\n",
       " '3701',\n",
       " '371',\n",
       " '372',\n",
       " '373',\n",
       " '375',\n",
       " '376',\n",
       " '379',\n",
       " '380',\n",
       " '382',\n",
       " '384',\n",
       " '386',\n",
       " '387',\n",
       " '390',\n",
       " '392',\n",
       " '398',\n",
       " '399',\n",
       " '39th',\n",
       " '3pm',\n",
       " '3rd',\n",
       " '400',\n",
       " '4000',\n",
       " '401',\n",
       " '403',\n",
       " '4035',\n",
       " '406',\n",
       " '407',\n",
       " '408',\n",
       " '4089',\n",
       " '40s',\n",
       " '410',\n",
       " '411',\n",
       " '413',\n",
       " '414',\n",
       " '415',\n",
       " '416',\n",
       " '419',\n",
       " '420',\n",
       " '421',\n",
       " '422',\n",
       " '423',\n",
       " '425',\n",
       " '428',\n",
       " '429',\n",
       " '430',\n",
       " '433',\n",
       " '4345',\n",
       " '435',\n",
       " '436',\n",
       " '437',\n",
       " '440',\n",
       " '441',\n",
       " '443',\n",
       " '445',\n",
       " '449',\n",
       " '450',\n",
       " '453',\n",
       " '455',\n",
       " '457',\n",
       " '459',\n",
       " '460',\n",
       " '461',\n",
       " '464',\n",
       " '470',\n",
       " '473',\n",
       " '475b',\n",
       " '476',\n",
       " '477',\n",
       " '480',\n",
       " '482',\n",
       " '484',\n",
       " '486',\n",
       " '491',\n",
       " '493',\n",
       " '495',\n",
       " '499',\n",
       " '49ers',\n",
       " '4front',\n",
       " '4th',\n",
       " '500',\n",
       " '5000',\n",
       " '502',\n",
       " '5032',\n",
       " '5052',\n",
       " '506',\n",
       " '507',\n",
       " '509',\n",
       " '50k',\n",
       " '50m',\n",
       " '50s',\n",
       " '511',\n",
       " '512',\n",
       " '513',\n",
       " '5131',\n",
       " '517',\n",
       " '520',\n",
       " '521',\n",
       " '522',\n",
       " '523',\n",
       " '524',\n",
       " '525',\n",
       " '529',\n",
       " '530',\n",
       " '534',\n",
       " '535',\n",
       " '537',\n",
       " '538',\n",
       " '5394',\n",
       " '541',\n",
       " '542',\n",
       " '543',\n",
       " '5458',\n",
       " '549',\n",
       " '550',\n",
       " '556',\n",
       " '558',\n",
       " '560',\n",
       " '562',\n",
       " '567',\n",
       " '570',\n",
       " '572',\n",
       " '573',\n",
       " '574',\n",
       " '576',\n",
       " '577',\n",
       " '579',\n",
       " '585',\n",
       " '591',\n",
       " '593',\n",
       " '5940',\n",
       " '598',\n",
       " '5bn',\n",
       " '5pm',\n",
       " '5th',\n",
       " '600',\n",
       " '6000',\n",
       " '602',\n",
       " '604',\n",
       " '6048',\n",
       " '606',\n",
       " '607',\n",
       " '609',\n",
       " '60s',\n",
       " '611',\n",
       " '612',\n",
       " '613',\n",
       " '617',\n",
       " '619',\n",
       " '620',\n",
       " '621',\n",
       " '622',\n",
       " '624',\n",
       " '625',\n",
       " '627',\n",
       " '629',\n",
       " '630',\n",
       " '634',\n",
       " '636',\n",
       " '637',\n",
       " '643',\n",
       " '645',\n",
       " '650',\n",
       " '653',\n",
       " '655',\n",
       " '656',\n",
       " '664',\n",
       " '665',\n",
       " '667',\n",
       " '670',\n",
       " '677',\n",
       " '678',\n",
       " '67m',\n",
       " '680',\n",
       " '682',\n",
       " '686',\n",
       " '688',\n",
       " '693',\n",
       " '699',\n",
       " '6pm',\n",
       " '6th',\n",
       " '700',\n",
       " '7000',\n",
       " '702',\n",
       " '70s',\n",
       " '70th',\n",
       " '710',\n",
       " '711',\n",
       " '718',\n",
       " '720',\n",
       " '722',\n",
       " '725',\n",
       " '726',\n",
       " '728',\n",
       " '730',\n",
       " '732',\n",
       " '735',\n",
       " '736',\n",
       " '740',\n",
       " '742',\n",
       " '748',\n",
       " '750',\n",
       " '755',\n",
       " '760',\n",
       " '7606',\n",
       " '761',\n",
       " '764',\n",
       " '766',\n",
       " '770',\n",
       " '773',\n",
       " '774',\n",
       " '776',\n",
       " '778',\n",
       " '780',\n",
       " '782',\n",
       " '783',\n",
       " '788',\n",
       " '789',\n",
       " '791',\n",
       " '792',\n",
       " '797',\n",
       " '799',\n",
       " '7acres',\n",
       " '7pm',\n",
       " '7th',\n",
       " '800',\n",
       " '8000',\n",
       " '800m',\n",
       " '802',\n",
       " '803',\n",
       " '805',\n",
       " '806',\n",
       " '807',\n",
       " '80s',\n",
       " '8155',\n",
       " '816',\n",
       " '819',\n",
       " '820',\n",
       " '823',\n",
       " '826',\n",
       " '830',\n",
       " '833',\n",
       " '834',\n",
       " '835',\n",
       " '837',\n",
       " '839',\n",
       " '841',\n",
       " '843',\n",
       " '845',\n",
       " '846',\n",
       " '847',\n",
       " '849',\n",
       " '850',\n",
       " '852',\n",
       " '854',\n",
       " '855',\n",
       " '859',\n",
       " '861',\n",
       " '863',\n",
       " '864',\n",
       " '865',\n",
       " '8676',\n",
       " '869',\n",
       " '872',\n",
       " '873',\n",
       " '877',\n",
       " '880',\n",
       " '885',\n",
       " '888',\n",
       " '890',\n",
       " '897',\n",
       " '898',\n",
       " '8pm',\n",
       " '8th',\n",
       " '900',\n",
       " '903',\n",
       " '907',\n",
       " '908',\n",
       " '90s',\n",
       " '910',\n",
       " '911',\n",
       " '912',\n",
       " '915',\n",
       " '917',\n",
       " '922',\n",
       " '928',\n",
       " '929',\n",
       " '930',\n",
       " '934',\n",
       " '939',\n",
       " '940',\n",
       " '946',\n",
       " '947',\n",
       " '949',\n",
       " '950',\n",
       " '954',\n",
       " '959',\n",
       " '960',\n",
       " '965',\n",
       " '967',\n",
       " '970',\n",
       " '975',\n",
       " '990',\n",
       " '994',\n",
       " '996',\n",
       " '999',\n",
       " '99th',\n",
       " '9am',\n",
       " '9pm',\n",
       " '9th',\n",
       " 'aaa',\n",
       " 'aamc',\n",
       " 'aaron',\n",
       " 'aarp',\n",
       " 'aba',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abate',\n",
       " 'abatement',\n",
       " 'abating',\n",
       " 'abattis',\n",
       " 'abba',\n",
       " 'abbotsford',\n",
       " 'abbott',\n",
       " 'abbreviated',\n",
       " 'abbreviation',\n",
       " 'abbvie',\n",
       " 'abc',\n",
       " 'abc15',\n",
       " 'abc7',\n",
       " 'abcann',\n",
       " 'abdominal',\n",
       " 'abdul',\n",
       " 'abel',\n",
       " 'abetted',\n",
       " 'abetting',\n",
       " 'abi',\n",
       " 'abide',\n",
       " 'abides',\n",
       " 'abiding',\n",
       " 'abigail',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abington',\n",
       " 'abject',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'abner',\n",
       " 'abnormal',\n",
       " 'abnormalities',\n",
       " 'aboard',\n",
       " 'abolish',\n",
       " 'abolished',\n",
       " 'abolishment',\n",
       " 'aboriginal',\n",
       " 'abortion',\n",
       " 'abound',\n",
       " 'abp',\n",
       " 'abraham',\n",
       " 'abrami',\n",
       " 'abrams',\n",
       " 'abreast',\n",
       " 'abro',\n",
       " 'abroad',\n",
       " 'abrupt',\n",
       " 'absence',\n",
       " 'absences',\n",
       " 'absent',\n",
       " 'absentee',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorbing',\n",
       " 'absorption',\n",
       " 'abstain',\n",
       " 'abstained',\n",
       " 'abstaining',\n",
       " 'abstentions',\n",
       " 'abstinence',\n",
       " 'abstract',\n",
       " 'abstracts',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'abta',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abundantly',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuses',\n",
       " 'abusing',\n",
       " 'abusive',\n",
       " 'aca',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academics',\n",
       " 'academies',\n",
       " 'academy',\n",
       " 'acadia',\n",
       " 'acb',\n",
       " 'acbff',\n",
       " 'acc',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accelerates',\n",
       " 'accelerating',\n",
       " 'accelerator',\n",
       " 'accelerators',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessed',\n",
       " 'accesses',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessing',\n",
       " 'accessories',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accidently',\n",
       " 'accidents',\n",
       " 'acclaimed',\n",
       " 'accommodate',\n",
       " 'accommodated',\n",
       " 'accommodating',\n",
       " 'accommodation',\n",
       " 'accommodations',\n",
       " 'accompanied',\n",
       " 'accompanies',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accompli',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishing',\n",
       " 'accomplishment',\n",
       " 'accord',\n",
       " 'accordance',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accountant',\n",
       " 'accountants',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'accreditation',\n",
       " 'accredited',\n",
       " 'accrual',\n",
       " 'accrue',\n",
       " 'accrued',\n",
       " 'accumulate',\n",
       " 'accumulating',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accusing',\n",
       " 'accustomed',\n",
       " 'aceso',\n",
       " 'aceto',\n",
       " 'aches',\n",
       " 'acheson',\n",
       " 'achievable',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achieves',\n",
       " 'achieving',\n",
       " 'aching',\n",
       " 'acid',\n",
       " 'acidic',\n",
       " 'acids',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledgement',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acknowledgment',\n",
       " 'acl',\n",
       " 'aclu',\n",
       " 'acmd',\n",
       " 'acmpr',\n",
       " 'acne',\n",
       " 'acosta',\n",
       " 'acoustic',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquirer',\n",
       " 'acquires',\n",
       " 'acquiring',\n",
       " 'acquisition',\n",
       " 'acquisitions',\n",
       " 'acquitted',\n",
       " 'acre',\n",
       " 'acreage',\n",
       " 'acreageholdings',\n",
       " 'acreages',\n",
       " 'acres',\n",
       " 'acronym',\n",
       " 'acs',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actionable',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'activated',\n",
       " 'activates',\n",
       " 'activating',\n",
       " 'activation',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activism',\n",
       " 'activist',\n",
       " 'activists',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'acton',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'acumen',\n",
       " 'acupuncture',\n",
       " 'acupuncturist',\n",
       " 'acute',\n",
       " 'ada',\n",
       " 'adage',\n",
       " 'adam',\n",
       " 'adamant',\n",
       " 'adamantly',\n",
       " 'adams',\n",
       " 'adapt',\n",
       " 'adaptable',\n",
       " ...]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Build LDA model with sklearn</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda =  LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "cannabis marijuana medical said state hemp city use new patients states people federal law business legal recreational industry drug california health dispensaries products government cbd legalization canada company year businesses market like research police public dispensary laws county time sales license tax program says department percent pot oregon oil medicinal\n",
      "Topic 1:\n",
      "les mydx que hausman quatre vins yazbeck bedard analyzer pour avec oprah est ete enrico qui aerodx spannabis heins handheld une loi vthc jaremowich leur sin este sur embargo fin sensors cynthea flavours surete toujours aerien controle canadienne sujet maintenant creme addison par reneged futura middlebury bouchard hay psoe sparkling\n",
      "Topic 2:\n",
      "420 news intel industry marijuana outlet technological impact developments cover world date information stories advances cannabis pertinent constantly rallies evolving reliable abreast inbox signing ensuring delivered globe kept legalization changing directly day daily offer endeavor owners develop carries internationally regarding constant regionally nationally benefits coverage need medicinal source development understand\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Show Dominant topic of each document</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lda_model=lda\n",
    "data_vectorized=tfidf\n",
    "# Create Document â€” Topic Matrix\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(dataset))]\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>List first 10 documents their dominent topic is 2</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic0</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Doc106</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc134</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc191</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc203</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc214</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc366</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc378</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc598</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc608</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc622</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Topic0  Topic1  Topic2  dominant_topic\n",
       "Doc106    0.07    0.05    0.89               2\n",
       "Doc134    0.06    0.05    0.89               2\n",
       "Doc191    0.06    0.05    0.89               2\n",
       "Doc203    0.07    0.05    0.88               2\n",
       "Doc214    0.06    0.05    0.89               2\n",
       "Doc366    0.07    0.05    0.88               2\n",
       "Doc378    0.07    0.05    0.88               2\n",
       "Doc598    0.07    0.05    0.89               2\n",
       "Doc608    0.07    0.05    0.89               2\n",
       "Doc622    0.06    0.05    0.89               2"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_document_topic[df_document_topic[\"dominant_topic\"]==2].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>List first 10 documents their dominent topic is 1</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic0</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Doc7619</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc7671</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc7681</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc9931</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Topic0  Topic1  Topic2  dominant_topic\n",
       "Doc7619    0.44    0.49    0.06               1\n",
       "Doc7671    0.18    0.76    0.06               1\n",
       "Doc7681    0.21    0.73    0.06               1\n",
       "Doc9931    0.26    0.66    0.08               1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_document_topic[df_document_topic[\"dominant_topic\"]==1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>List first 10 documents their dominent topic is 0</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic0</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Doc0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc1</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc2</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc3</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc4</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc5</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc6</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc7</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc8</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc9</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic0  Topic1  Topic2  dominant_topic\n",
       "Doc0    0.92    0.04    0.04               0\n",
       "Doc1    0.94    0.03    0.03               0\n",
       "Doc2    0.92    0.04    0.04               0\n",
       "Doc3    0.33    0.33    0.33               0\n",
       "Doc4    0.93    0.03    0.03               0\n",
       "Doc5    0.86    0.07    0.07               0\n",
       "Doc6    0.94    0.03    0.03               0\n",
       "Doc7    0.93    0.03    0.03               0\n",
       "Doc8    0.92    0.04    0.04               0\n",
       "Doc9    0.91    0.05    0.05               0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_document_topic[df_document_topic[\"dominant_topic\"]==0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer, lda_model, n_words=10):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Top 15 words of each topic</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Topic 0</td>\n",
       "      <td>cannabis</td>\n",
       "      <td>marijuana</td>\n",
       "      <td>medical</td>\n",
       "      <td>said</td>\n",
       "      <td>state</td>\n",
       "      <td>hemp</td>\n",
       "      <td>city</td>\n",
       "      <td>use</td>\n",
       "      <td>new</td>\n",
       "      <td>patients</td>\n",
       "      <td>states</td>\n",
       "      <td>people</td>\n",
       "      <td>federal</td>\n",
       "      <td>law</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 1</td>\n",
       "      <td>les</td>\n",
       "      <td>mydx</td>\n",
       "      <td>que</td>\n",
       "      <td>hausman</td>\n",
       "      <td>quatre</td>\n",
       "      <td>vins</td>\n",
       "      <td>yazbeck</td>\n",
       "      <td>bedard</td>\n",
       "      <td>analyzer</td>\n",
       "      <td>pour</td>\n",
       "      <td>avec</td>\n",
       "      <td>oprah</td>\n",
       "      <td>est</td>\n",
       "      <td>ete</td>\n",
       "      <td>enrico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 2</td>\n",
       "      <td>420</td>\n",
       "      <td>news</td>\n",
       "      <td>intel</td>\n",
       "      <td>industry</td>\n",
       "      <td>marijuana</td>\n",
       "      <td>outlet</td>\n",
       "      <td>technological</td>\n",
       "      <td>impact</td>\n",
       "      <td>developments</td>\n",
       "      <td>cover</td>\n",
       "      <td>world</td>\n",
       "      <td>date</td>\n",
       "      <td>information</td>\n",
       "      <td>stories</td>\n",
       "      <td>advances</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word 0     Word 1   Word 2    Word 3     Word 4  Word 5  \\\n",
       "Topic 0  cannabis  marijuana  medical      said      state    hemp   \n",
       "Topic 1       les       mydx      que   hausman     quatre    vins   \n",
       "Topic 2       420       news    intel  industry  marijuana  outlet   \n",
       "\n",
       "                Word 6  Word 7        Word 8    Word 9 Word 10 Word 11  \\\n",
       "Topic 0           city     use           new  patients  states  people   \n",
       "Topic 1        yazbeck  bedard      analyzer      pour    avec   oprah   \n",
       "Topic 2  technological  impact  developments     cover   world    date   \n",
       "\n",
       "             Word 12  Word 13   Word 14  \n",
       "Topic 0      federal      law  business  \n",
       "Topic 1          est      ete    enrico  \n",
       "Topic 2  information  stories  advances  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_keywords = show_topics(tfidf_vectorizer, lda, 15)\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Okay lets assign some labels for each topic based on words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Topic 0</td>\n",
       "      <td>cannabis</td>\n",
       "      <td>marijuana</td>\n",
       "      <td>medical</td>\n",
       "      <td>said</td>\n",
       "      <td>state</td>\n",
       "      <td>hemp</td>\n",
       "      <td>city</td>\n",
       "      <td>use</td>\n",
       "      <td>new</td>\n",
       "      <td>patients</td>\n",
       "      <td>states</td>\n",
       "      <td>people</td>\n",
       "      <td>federal</td>\n",
       "      <td>law</td>\n",
       "      <td>business</td>\n",
       "      <td>Drug Use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 1</td>\n",
       "      <td>les</td>\n",
       "      <td>mydx</td>\n",
       "      <td>que</td>\n",
       "      <td>hausman</td>\n",
       "      <td>quatre</td>\n",
       "      <td>vins</td>\n",
       "      <td>yazbeck</td>\n",
       "      <td>bedard</td>\n",
       "      <td>analyzer</td>\n",
       "      <td>pour</td>\n",
       "      <td>avec</td>\n",
       "      <td>oprah</td>\n",
       "      <td>est</td>\n",
       "      <td>ete</td>\n",
       "      <td>enrico</td>\n",
       "      <td>Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 2</td>\n",
       "      <td>420</td>\n",
       "      <td>news</td>\n",
       "      <td>intel</td>\n",
       "      <td>industry</td>\n",
       "      <td>marijuana</td>\n",
       "      <td>outlet</td>\n",
       "      <td>technological</td>\n",
       "      <td>impact</td>\n",
       "      <td>developments</td>\n",
       "      <td>cover</td>\n",
       "      <td>world</td>\n",
       "      <td>date</td>\n",
       "      <td>information</td>\n",
       "      <td>stories</td>\n",
       "      <td>advances</td>\n",
       "      <td>Technological Developments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word 0     Word 1   Word 2    Word 3     Word 4  Word 5  \\\n",
       "Topic 0  cannabis  marijuana  medical      said      state    hemp   \n",
       "Topic 1       les       mydx      que   hausman     quatre    vins   \n",
       "Topic 2       420       news    intel  industry  marijuana  outlet   \n",
       "\n",
       "                Word 6  Word 7        Word 8    Word 9 Word 10 Word 11  \\\n",
       "Topic 0           city     use           new  patients  states  people   \n",
       "Topic 1        yazbeck  bedard      analyzer      pour    avec   oprah   \n",
       "Topic 2  technological  impact  developments     cover   world    date   \n",
       "\n",
       "             Word 12  Word 13   Word 14                    Category  \n",
       "Topic 0      federal      law  business                    Drug Use  \n",
       "Topic 1          est      ete    enrico                    Research  \n",
       "Topic 2  information  stories  advances  Technological Developments  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Topics = [\"Drug Use\",\"Research\",\"Technological Developments\"]\n",
    "df_topic_keywords[\"Category\"]=Topics\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
